{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Conv2DTranspose, Reshape, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories\n",
    "train_data_dir = \"Dataset/Preprocessed/Tomato/train_set\"\n",
    "test_data_dir = \"Dataset/Preprocessed/Tomato/test_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions and batch size\n",
    "img_width, img_height = 400, 400\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode=\"categorical\", shuffle=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode=\"categorical\", shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing pretrained ResNet-50 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model\n",
    "base_resNet_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_resNet_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "resnet_model = Model(inputs=base_resNet_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in base_resNet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "resnet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = resnet_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=10, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "resnet_model.save(\"resNet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing DenseNet-121 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DenseNet121 model\n",
    "base_denseNet_model = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_denseNet_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "denseNet_model = Model(inputs=base_denseNet_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in base_denseNet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "denseNet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = denseNet_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=10, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "denseNet_model.save(\"denseNet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing custom CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN architecture\n",
    "custom_model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(5, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "custom_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = custom_model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=10, validation_data=test_generator, validation_steps=test_generator.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "custom_model.save(\"custom_CNN_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing custom GAN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generator\n",
    "# def build_generator(latent_dim):\n",
    "#     input_noise = Input(shape=(latent_dim,))\n",
    "#     x = Dense(7 * 7 * 128, activation=\"relu\")(input_noise)\n",
    "#     x = Reshape((7, 7, 128))(x)\n",
    "#     x = Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "#     x = Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "#     output_img = Conv2D(3, kernel_size=3, padding=\"same\", activation=\"tanh\")(x)\n",
    "#     generator = Model(input_noise, output_img)\n",
    "#     return generator\n",
    "\n",
    "# latent_dim = 100\n",
    "# generator = build_generator(latent_dim)\n",
    "\n",
    "\n",
    "# # Discriminator\n",
    "# def build_discriminator(input_shape):\n",
    "#     input_img = Input(shape=input_shape)\n",
    "#     x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(input_img)\n",
    "#     x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "#     x = Flatten()(x)\n",
    "#     output = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     discriminator = Model(input_img, output)\n",
    "#     discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "#     return discriminator\n",
    "\n",
    "# input_shape = (img_height, img_width, 3)  # Assuming 3 channels for RGB images\n",
    "# discriminator = build_discriminator(input_shape)\n",
    "\n",
    "\n",
    "# # Combined GAN model\n",
    "# discriminator.trainable = False\n",
    "# gan_input = Input(shape=(latent_dim,))\n",
    "# gan_output = discriminator(generator(gan_input))\n",
    "# gan = Model(gan_input, gan_output)\n",
    "# gan.compile(optimizer=Adam(learning_rate=0.0002), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training GAN\n",
    "# def train_gan(generator, discriminator, gan, train_generator, test_generator, latent_dim, epochs, batch_size):\n",
    "#     num_batches = len(train_generator)\n",
    "#     for epoch in range(epochs):\n",
    "#         for batch in range(num_batches):\n",
    "#             real_images, labels = next(train_generator)\n",
    "\n",
    "#             noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "#             fake_images = generator.predict(noise)\n",
    "\n",
    "#             discriminator_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
    "#             discriminator_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n",
    "#             discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "#             noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "#             gan_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "#             if batch % 100 == 0:\n",
    "#                 print(f\"Epoch: {epoch}, Batch: {batch}/{num_batches}, Discriminator Loss: {discriminator_loss}, GAN Loss: {gan_loss}\")\n",
    "\n",
    "#     # Evaluate GAN performance on test set\n",
    "#     test_loss, test_accuracy = gan.evaluate(test_generator)\n",
    "#     print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# train_gan(generator, discriminator, gan, train_generator, test_generator, latent_dim, epochs=20, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained generator model\n",
    "# generator.save(\"gan_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
